# -*- coding: utf-8 -*-
"""sentiment analysis_1(twitter).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RMIDSuiv5s0wUFuTgndpndACOHXzyiPC

### Sentiment analysis on twitter comments.

[dataset link](https://drive.google.com/file/d/12uAdA8VOkFbjXFJfgNfLtQzK-DvMznKJ/view?usp=sharing)

install necessary libraries
"""

!pip install pandas matplotlib tensorflow

"""download the dataset( dataset taken here is for twitter)"""

import pandas as pd
df = pd.read_csv("/content/Tweets.csv")

print(df)

review_df = df[['text','airline_sentiment']]

print(review_df.shape)
review_df.head(5)

df.columns

review_df = review_df[review_df['airline_sentiment'] != 'neutral']

print(review_df.shape)
review_df.head(5)

review_df["airline_sentiment"].value_counts() #check the values of airline sentiment column

"""factorise method - converting the categorical values into numeric values so that the machine can understand."""

sentiment_label = review_df.airline_sentiment.factorize()
sentiment_label

tweet = review_df.text.values

"""tokenization: breaking down all the parts of text into small parts called tokens."""

from tensorflow.keras.preprocessing.text import Tokenizer

tokenizer = Tokenizer(num_words=5000)

tokenizer.fit_on_texts(tweet)

"""fit on texts in converted into texts to sequenence."""

encoded_docs = tokenizer.texts_to_sequences(tweet) #words which were replaced are assigned with their numbers using text_to_sequence()method/

"""sentences in dataset maynot be of equal length so use padding to pad the sentences to have equal lengths."""

from tensorflow.keras.preprocessing.sequence import pad_sequences

padded_sequence = pad_sequences(encoded_docs, maxlen=200)

"""build a lstm (long short text memory) a variant of rnn(recurrent neural netwroks)

#Spatial drop out:
type of drop out used in cnn.

# What actually is dropout?
technique that randomly drops out some elements of a layer during training. this widely helps the model to overcome the problem of overfitting.

# What is overfitting?
it is the case when machine or model learns well from the training data, but will be unable to generalize to any new data provided.
methods followed to overcome this drawback is:
 1. regularization
 2. data augmentation(refers to the altering or modifying the data interms of different parameters).
 3. early stopping ( it refers to stopping the training process before the model starts to verfit the data.)
"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM,Dense, Dropout, SpatialDropout1D
from tensorflow.keras.layers import Embedding
vocab_size = 13234    # this changed the final results(graphs) of exsisting ones
embedding_vector_length = 32
model = Sequential()
model.add(Embedding(vocab_size,embedding_vector_length, input_length=200))
model.add(SpatialDropout1D(0.25))
model.add(LSTM(50, dropout=0.5, recurrent_dropout=0.5))
model.add(Dropout(0.2))
model.add(Dense(1, activation='sigmoid'))
model.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])

print(model.summary())

"""train your model now

epoches refers to one completete pass through the entire dataset.
this means that the model sees the everydata point in the training dataset once.
no of epochs is a hyperparameter that can be tuned to improve the perforamnce of the model.

note: once ur model starts to overfit see through u reduce the number of epochs or use techniques to prevent overfitting.
"""

history = model.fit(padded_sequence,sentiment_label[0],validation_split=0.2, epochs=5, batch_size=32)

"""plot the data using matplotlib"""

import matplotlib.pyplot as plt

plt.plot(history.history['accuracy'], label='acc')
plt.plot(history.history['val_accuracy'], label='val_acc')
plt.legend()
plt.show()

plt.savefig("Accuracy plot.jpg")

plt.plot(history.history['loss'], label='loss')
plt.plot(history.history['val_loss'], label='val_loss')

plt.legend()
plt.show()

plt.savefig("Loss plt.jpg")

"""give your statements and check for the tone of the comment"""

#executing the model
def predict_sentiment(text):
    tw = tokenizer.texts_to_sequences([text])
    tw = pad_sequences(tw,maxlen=200)
    prediction = int(model.predict(tw).round().item())
    print("Predicted label: ", sentiment_label[1][prediction])


test_sentence1 = "I enjoyed my journey on this flight."
predict_sentiment(test_sentence1)

test_sentence2 = "bags were not proper they were torn while receiving us."
predict_sentiment(test_sentence2)

test_sentence3 = "one of the best flight i had ever."
predict_sentiment(test_sentence3)

test_sentence4 = "the worst airline i ever had, service was very poor"
predict_sentiment(test_sentence4)